"""
AI-Powered Sugya Extraction System
Uses OpenAI GPT-4 to analyze Talmudic texts and extract sugyot
"""
import os
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import json
import re
from database import get_driver

class SugyaExtractor:
    """
    AI-powered system to automatically identify and extract sugyot from Talmudic texts.
    
    Uses GPT-4 to:
    - Identify sugya boundaries (where one topic ends and another begins)
    - Extract main themes and questions
    - Analyze dialectic structure
    - Generate titles and summaries
    """
    
    def __init__(self):
        self.driver = get_driver()
        api_key = os.getenv("OPENAI_API_KEY")
        
        if not api_key or api_key == "sk-your-openai-api-key-here":
            print("âš ï¸  Warning: OPENAI_API_KEY not set. AI extraction will be simulated.")
            self.client = None
        else:
            self.client = OpenAI(api_key=api_key)
            print("âœ… OpenAI client initialized for sugya extraction")
    
    def discover_all_tractates(self) -> List[str]:
        """
        Discover all Talmudic tractates that have texts in the database.
        
        Returns:
            List of tractate names found in the database
        """
        with self.driver.session() as session:
            query = """
            MATCH (t:Text)
            WHERE t.id =~ '.*\\d+[ab]:.*'
            WITH split(t.id, ' ') as parts
            WHERE size(parts) > 1
            WITH parts[0] as tractate
            RETURN DISTINCT tractate
            ORDER BY tractate
            """
            
            result = session.run(query)
            tractates = [record['tractate'] for record in result]
            
            return tractates
    
    def extract_all_sugyot(self, limit_per_tractate: int = 100) -> Dict:
        """
        Discover and extract ALL sugyot from ALL tractates in the database.
        
        This is the main entry point for complete automatic extraction.
        
        Args:
            limit_per_tractate: Maximum texts to analyze per tractate
        
        Returns:
            Statistics about the complete extraction
        """
        print("=" * 80)
        print("ðŸŒ DISCOVERING ALL TRACTATES IN DATABASE")
        print("=" * 80)
        
        # Discover all tractates
        tractates = self.discover_all_tractates()
        print(f"\nFound {len(tractates)} tractates with Talmudic texts:")
        for tractate in tractates:
            print(f"  - {tractate}")
        
        if not tractates:
            print("\nâŒ No tractates found in database")
            return {
                'tractates_found': 0,
                'tractates_processed': 0,
                'total_extracted': 0,
                'total_saved': 0,
                'total_failed': 0
            }
        
        print("\n" + "=" * 80)
        print("ðŸš€ STARTING EXTRACTION FROM ALL TRACTATES")
        print("=" * 80)
        
        # Extract from each tractate
        all_stats = {
            'tractates_found': len(tractates),
            'tractates_processed': 0,
            'total_extracted': 0,
            'total_saved': 0,
            'total_failed': 0,
            'tractate_details': []
        }
        
        for i, tractate in enumerate(tractates, 1):
            print(f"\n{'=' * 80}")
            print(f"ðŸ“– TRACTATE {i}/{len(tractates)}: {tractate}")
            print(f"{'=' * 80}")
            
            try:
                stats = self.extract_and_save_all(
                    tractate=tractate,
                    start_page="",  # Empty means all pages
                    limit=limit_per_tractate
                )
                
                all_stats['tractates_processed'] += 1
                all_stats['total_extracted'] += stats['total_extracted']
                all_stats['total_saved'] += stats['saved']
                all_stats['total_failed'] += stats['failed']
                all_stats['tractate_details'].append({
                    'tractate': tractate,
                    'extracted': stats['total_extracted'],
                    'saved': stats['saved']
                })
                
                print(f"\nâœ… {tractate}: Extracted {stats['total_extracted']}, Saved {stats['saved']}")
                
            except Exception as e:
                print(f"\nâŒ Error processing {tractate}: {e}")
                all_stats['tractate_details'].append({
                    'tractate': tractate,
                    'error': str(e)
                })
        
        return all_stats
    
    def extract_sugyot_from_tractate(
        self, 
        tractate: str = "Berakhot",
        start_page: str = "2a",
        end_page: Optional[str] = None,
        limit: int = 50
    ) -> List[Dict]:
        """
        Extract sugyot from a Talmudic tractate using AI analysis.
        
        Args:
            tractate: Name of tractate (e.g., "Berakhot")
            start_page: Starting page (e.g., "2a")
            end_page: Ending page (optional)
            limit: Maximum number of texts to analyze
        
        Returns:
            List of extracted sugyot with AI-generated metadata
        """
        print(f"\nðŸ” Extracting sugyot from {tractate} {start_page}...")
        
        # Step 1: Fetch texts from database
        texts = self._fetch_texts(tractate, start_page, limit)
        print(f"   Found {len(texts)} texts to analyze")
        
        if not texts:
            return []
        
        # Step 2: Group texts by page for context
        pages = self._group_texts_by_page(texts)
        print(f"   Grouped into {len(pages)} pages")
        
        # Step 3: Analyze each page with AI
        extracted_sugyot = []
        for page_ref, page_texts in pages.items():
            print(f"\n   Analyzing {page_ref}...")
            
            # Combine texts for analysis
            combined_content = self._combine_texts(page_texts)
            
            # Use AI to analyze the sugya
            sugya_data = self._analyze_sugya_with_ai(page_ref, combined_content)
            
            if sugya_data:
                sugya_data['texts'] = page_texts
                extracted_sugyot.append(sugya_data)
                print(f"   âœ… Extracted: {sugya_data['title']}")
        
        return extracted_sugyot
    
    def _fetch_texts(self, tractate: str, page: str, limit: int) -> List[Dict]:
        """Fetch texts from Neo4j database"""
        with self.driver.session() as session:
            # Build query based on parameters
            if page:
                # Specific page
                query = """
                MATCH (t:Text)
                WHERE t.id CONTAINS $tractate 
                AND t.id CONTAINS $page
                RETURN t.id as id,
                       t.content_he as content_he,
                       t.content_en as content_en
                ORDER BY t.id
                LIMIT $limit
                """
                params = {"tractate": tractate, "page": page, "limit": limit}
            else:
                # All pages from tractate
                query = """
                MATCH (t:Text)
                WHERE t.id STARTS WITH $tractate
                AND t.id =~ '.*\\d+[ab]:.*'
                RETURN t.id as id,
                       t.content_he as content_he,
                       t.content_en as content_en
                ORDER BY t.id
                LIMIT $limit
                """
                params = {"tractate": tractate + " ", "limit": limit}
            
            result = session.run(query, params)
            
            texts = []
            for record in result:
                texts.append({
                    'id': record['id'],
                    'content_he': record.get('content_he', ''),
                    'content_en': record.get('content_en', [])
                })
            
            return texts
    
    def _group_texts_by_page(self, texts: List[Dict]) -> Dict[str, List[Dict]]:
        """Group texts by page reference"""
        pages = {}
        
        for text in texts:
            text_id = text['id']
            
            # Extract page reference (e.g., "2a", "2b")
            page_match = re.search(r'(\d+[ab])', text_id)
            if page_match:
                page = page_match.group(1)
                
                # Extract tractate name
                tractate_match = re.search(r'([A-Za-z]+)\s+\d+[ab]', text_id)
                tractate = tractate_match.group(1) if tractate_match else "Unknown"
                
                page_ref = f"{tractate} {page}"
                
                if page_ref not in pages:
                    pages[page_ref] = []
                
                pages[page_ref].append(text)
        
        return pages
    
    def _combine_texts(self, texts: List[Dict]) -> str:
        """Combine multiple text nodes into a single string for analysis"""
        combined = []
        
        for text in texts:
            content = text.get('content_he', '')
            
            # Handle list content
            if isinstance(content, list):
                content = ' '.join(str(c) for c in content if c)
            
            # Remove HTML tags
            content = re.sub(r'<[^>]+>', '', str(content))
            
            if content.strip():
                combined.append(content.strip())
        
        return '\n\n'.join(combined)
    
    def _analyze_sugya_with_ai(self, page_ref: str, content: str) -> Optional[Dict]:
        """
        Use AI to analyze a sugya and extract its structure.
        
        Returns:
            Dict with title, summary, theme, main_question, and dialectic structure
        """
        if not self.client:
            # Simulate AI response when no API key
            return self._simulate_ai_analysis(page_ref, content)
        
        try:
            # Prepare the prompt
            prompt = self._create_analysis_prompt(page_ref, content)
            
            # Call GPT-4
            response = self.client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert in Talmudic literature and dialectic analysis. Analyze the given sugya and extract its structure."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=1500
            )
            
            # Parse the response
            result = response.choices[0].message.content
            sugya_data = self._parse_ai_response(page_ref, result)
            
            return sugya_data
            
        except Exception as e:
            print(f"   âš ï¸  AI analysis failed: {e}")
            return self._simulate_ai_analysis(page_ref, content)
    
    def _create_analysis_prompt(self, page_ref: str, content: str) -> str:
        """Create a prompt for AI analysis"""
        # Truncate content if too long
        max_length = 4000
        if len(content) > max_length:
            content = content[:max_length] + "..."
        
        return f"""Analyze this Talmudic sugya from {page_ref} and extract ALL dialectic steps.

TEXT:
{content}

Please provide a COMPLETE analysis with:
1. A concise title (5-10 words) that captures the main topic
2. A one-sentence summary
3. The main theme or question being discussed
4. The COMPLETE dialectic structure - extract ALL steps, statements, and arguments

For the dialectic_nodes array, include EVERY step of the sugya:
- EVERY question (kasha, kushya, teyuvta)
- EVERY answer (terutz, peshat, teshuvah)
- EVERY teaching (mishnah, braita, statement)
- EVERY dispute (machloket, pluga)
- EVERY challenge and resolution
- EVERY proof and refutation
- ALL intermediate steps

For each node provide:
- id: sequential number (1, 2, 3, ...)
- type: "question", "answer", "kasha", "terutz", "mishnah", "braita", "statement", "dispute", "proof", "refutation", "conclusion", "teiku"
- label: clear description of this step (50-100 characters)
- speaker: who is speaking (Mishnah, Gemara, specific rabbi if mentioned)
- content_preview: first 30-50 words from the actual text
- parent_id: ID of the step this responds to (null for first step)

IMPORTANT: Extract as many nodes as possible - aim for 10-20+ nodes per sugya to capture the complete dialectic flow.

Format your response as JSON:
{{
    "title": "...",
    "summary": "...",
    "main_question": "...",
    "theme": "...",
    "dialectic_nodes": [
        {{
            "id": "1",
            "type": "mishnah",
            "label": "Initial teaching from Mishnah",
            "speaker": "Mishnah",
            "content_preview": "...",
            "parent_id": null
        }},
        {{
            "id": "2",
            "type": "question",
            "label": "Gemara asks for clarification",
            "speaker": "Gemara",
            "content_preview": "...",
            "parent_id": "1"
        }},
        {{
            "id": "3",
            "type": "answer",
            "label": "Response explaining the timing",
            "speaker": "R. Yochanan",
            "content_preview": "...",
            "parent_id": "2"
        }},
        ... (continue with ALL steps)
    ]
}}"""
    
    def _parse_ai_response(self, page_ref: str, response: str) -> Dict:
        """Parse AI response into structured data"""
        try:
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(0))
                
                # Build the sugya structure
                return {
                    'ref': page_ref,
                    'title': data.get('title', f'Discussion on {page_ref}'),
                    'summary': data.get('summary', ''),
                    'theme': data.get('theme', ''),
                    'main_question': data.get('main_question', ''),
                    'dialectic_nodes': data.get('dialectic_nodes', [])
                }
        except Exception as e:
            print(f"   âš ï¸  Failed to parse AI response: {e}")
        
        # Fallback to basic structure
        return self._simulate_ai_analysis(page_ref, '')
    
    def _simulate_ai_analysis(self, page_ref: str, content: str) -> Dict:
        """Simulate AI analysis when no API key is available"""
        # Extract multiple lines and create more detailed structure
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        
        # Build a more comprehensive simulated structure
        nodes = []
        
        # Create nodes from content lines (simulate extracting steps)
        for i, line in enumerate(lines[:15], 1):  # Take up to 15 lines
            # Determine type based on position and keywords
            if i == 1:
                node_type = 'mishnah'
                speaker = 'Mishnah'
            elif '?' in line or any(word in line for word in ['×œ×ž×”', '×ž××™', '×ž× ×', '×”×™×›×™']):
                node_type = 'kasha'
                speaker = 'Gemara'
            elif any(word in line for word in ['××ž×¨', '×ª× ×Ÿ', '×ª× ×™×']):
                node_type = 'statement'
                speaker = 'Gemara'
            elif any(word in line for word in ['×¤×œ×•×’×ª×', '×ž×—×œ×•×§×ª']):
                node_type = 'dispute'
                speaker = 'Talmud'
            elif i % 2 == 0:
                node_type = 'question'
                speaker = 'Gemara'
            else:
                node_type = 'answer'
                speaker = 'Gemara'
            
            # Clean and truncate label
            label = line[:80].strip()
            if len(line) > 80:
                label += '...'
            
            nodes.append({
                'id': str(i),
                'type': node_type,
                'label': label,
                'speaker': speaker,
                'content_preview': line[:100],
                'parent_id': str(i-1) if i > 1 else None
            })
        
        # If we have very few lines, add generic nodes
        if len(nodes) < 5:
            base_count = len(nodes)
            additional = [
                {'id': str(base_count + 1), 'type': 'kasha', 'label': 'Challenge to the statement', 'speaker': 'Gemara', 'content_preview': '', 'parent_id': str(base_count)},
                {'id': str(base_count + 2), 'type': 'terutz', 'label': 'Resolution of the challenge', 'speaker': 'Gemara', 'content_preview': '', 'parent_id': str(base_count + 1)},
                {'id': str(base_count + 3), 'type': 'proof', 'label': 'Proof from another source', 'speaker': 'Gemara', 'content_preview': '', 'parent_id': str(base_count + 2)},
                {'id': str(base_count + 4), 'type': 'conclusion', 'label': 'Final ruling', 'speaker': 'Gemara', 'content_preview': '', 'parent_id': str(base_count + 3)},
            ]
            nodes.extend(additional)
        
        first_line = lines[0][:100] if lines else "Discussion topic"
        
        return {
            'ref': page_ref,
            'title': f'Discussion on {page_ref}',
            'summary': f'Talmudic discussion from {page_ref}',
            'theme': 'Halakhic discourse',
            'main_question': first_line,
            'dialectic_nodes': nodes
        }
    
    def save_sugya_to_database(self, sugya_data: Dict) -> bool:
        """
        Save extracted sugya to Neo4j database.
        Creates Sugya node and dialectic structure.
        """
        with self.driver.session() as session:
            try:
                # Create Sugya node
                query = """
                MERGE (s:Sugya {ref: $ref})
                SET s.title = $title,
                    s.summary = $summary,
                    s.theme = $theme,
                    s.main_question = $main_question,
                    s.extraction_method = 'ai_powered',
                    s.updated_at = datetime()
                WITH s
                WHERE s.created_at IS NULL
                SET s.created_at = datetime()
                RETURN s
                """
                
                session.run(query, {
                    'ref': sugya_data['ref'],
                    'title': sugya_data['title'],
                    'summary': sugya_data['summary'],
                    'theme': sugya_data.get('theme', ''),
                    'main_question': sugya_data.get('main_question', '')
                })
                
                # Link to Text nodes
                link_query = """
                MATCH (s:Sugya {ref: $ref})
                MATCH (t:Text)
                WHERE t.id CONTAINS $ref
                MERGE (s)-[:CONTAINS_TEXT]->(t)
                """
                
                session.run(link_query, {'ref': sugya_data['ref']})
                
                # Create dialectic nodes
                if sugya_data.get('dialectic_nodes'):
                    self._create_dialectic_nodes(session, sugya_data)
                
                return True
                
            except Exception as e:
                print(f"   âŒ Failed to save sugya: {e}")
                return False
    
    def _create_dialectic_nodes(self, session, sugya_data: Dict):
        """Create dialectic node structure in database with parent-child relationships"""
        nodes = sugya_data.get('dialectic_nodes', [])
        
        # First pass: Create all nodes
        for i, node in enumerate(nodes):
            query = """
            MATCH (s:Sugya {ref: $sugya_ref})
            MERGE (d:DialecticNode {
                id: $node_id,
                sugya_ref: $sugya_ref
            })
            SET d.type = $type,
                d.label = $label,
                d.speaker = $speaker,
                d.content_preview = $content_preview,
                d.sequence = $sequence,
                d.parent_id = $parent_id
            MERGE (s)-[:HAS_DIALECTIC_NODE]->(d)
            """
            
            session.run(query, {
                'sugya_ref': sugya_data['ref'],
                'node_id': f"{sugya_data['ref']}-{node['id']}",
                'type': node.get('type', 'unknown'),
                'label': node.get('label', ''),
                'speaker': node.get('speaker', ''),
                'content_preview': node.get('content_preview', ''),
                'sequence': i + 1,
                'parent_id': node.get('parent_id', '')
            })
        
        # Second pass: Create parent-child relationships
        for node in nodes:
            parent_id = node.get('parent_id')
            if parent_id:
                link_query = """
                MATCH (parent:DialecticNode {id: $parent_full_id})
                MATCH (child:DialecticNode {id: $child_full_id})
                MERGE (parent)-[:LEADS_TO]->(child)
                """
                
                session.run(link_query, {
                    'parent_full_id': f"{sugya_data['ref']}-{parent_id}",
                    'child_full_id': f"{sugya_data['ref']}-{node['id']}"
                })
    
    def extract_and_save_all(
        self, 
        tractate: str = "Berakhot",
        start_page: str = "2a",
        limit: int = 50
    ) -> Dict:
        """
        Complete pipeline: Extract sugyot using AI and save to database.
        
        Returns:
            Statistics about the extraction process
        """
        print("=" * 80)
        print("AI-POWERED SUGYA EXTRACTION")
        print("=" * 80)
        
        # Extract sugyot
        sugyot = self.extract_sugyot_from_tractate(tractate, start_page, limit=limit)
        
        # Save to database
        saved_count = 0
        failed_count = 0
        
        print("\nðŸ’¾ Saving to database...")
        for sugya in sugyot:
            if self.save_sugya_to_database(sugya):
                saved_count += 1
                print(f"   âœ… Saved: {sugya['ref']} - {sugya['title']}")
            else:
                failed_count += 1
                print(f"   âŒ Failed: {sugya['ref']}")
        
        stats = {
            'total_extracted': len(sugyot),
            'saved': saved_count,
            'failed': failed_count,
            'tractate': tractate,
            'start_page': start_page
        }
        
        print("\n" + "=" * 80)
        print("EXTRACTION COMPLETE")
        print("=" * 80)
        print(f"Total extracted: {stats['total_extracted']}")
        print(f"Saved: {stats['saved']}")
        print(f"Failed: {stats['failed']}")
        
        return stats


def get_sugya_extractor() -> SugyaExtractor:
    """Get singleton SugyaExtractor instance"""
    return SugyaExtractor()

